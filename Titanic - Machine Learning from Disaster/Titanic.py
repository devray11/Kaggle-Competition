import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, 
                             GradientBoostingClassifier, AdaBoostClassifier, 
                             HistGradientBoostingClassifier, VotingClassifier, 
                             StackingClassifier)
from sklearn.svm import SVC, NuSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

print("üî• ULTIMATE 15+ MODEL TITANIC PIPELINE")
print("=" * 70)

# =============================================================================
# 1. ADVANCED FEATURE ENGINEERING
# =============================================================================
def advanced_features(df):
    df = df.copy()
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    df['LargeFamily'] = (df['FamilySize'] > 4).astype(int)
    
    df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)
    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 
                                       'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    df['Title'] = df['Title'].replace('Mlle', 'Miss')
    df['Title'] = df['Title'].replace('Ms', 'Miss')
    df['Title'] = df['Title'].replace('Mme', 'Mrs')
    
    df['HasCabin'] = df['Cabin'].notna().astype(int)
    df['CabinDeck'] = df['Cabin'].str[0].fillna('Missing')
    
    df['FareBin'] = pd.qcut(df['Fare'], 5, labels=False, duplicates='drop')
    df['AgeBin'] = pd.qcut(df['Age'].fillna(df['Age'].median()), 6, labels=False, duplicates='drop')
    
    df['Pclass_Sex'] = df['Pclass'].astype(str) + '_' + df['Sex']
    df['Age_FamilySize'] = df['Age'].fillna(0) * df['FamilySize']
    
    return df

train = pd.read_csv('Dataset/train.csv')
test = pd.read_csv('Dataset/test.csv')

y = train['Survived']
X_full = advanced_features(train.drop('Survived', axis=1))
test_full = advanced_features(test)
test_passenger_id = test['PassengerId']

numeric_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'Age_FamilySize']
categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'CabinDeck', 'Pclass_Sex']

preprocessor = ColumnTransformer([
    ('num', Pipeline([
        ('imputer', KNNImputer(n_neighbors=5)),
        ('scaler', StandardScaler())
    ]), numeric_features),
    
    ('cat', Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ]), categorical_features)
])

print(f"‚úÖ Advanced features created")

# =============================================================================
# 2. 15 ULTRA MODELS
# =============================================================================
base_models = {}

base_models['LR'] = LogisticRegression(C=0.1, max_iter=3000, random_state=42)
base_models['RF_500'] = RandomForestClassifier(n_estimators=500, max_depth=7, min_samples_split=3, 
                                               min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=-1)
base_models['ET_500'] = ExtraTreesClassifier(n_estimators=500, max_depth=7, min_samples_split=3, 
                                             random_state=42, n_jobs=-1)
base_models['DT'] = DecisionTreeClassifier(max_depth=8, min_samples_split=5, random_state=42)
base_models['GBC'] = GradientBoostingClassifier(n_estimators=500, max_depth=5, learning_rate=0.05, random_state=42)
base_models['HistGB'] = HistGradientBoostingClassifier(max_iter=500, max_depth=6, random_state=42)
base_models['XGB'] = XGBClassifier(n_estimators=800, max_depth=5, learning_rate=0.03, subsample=0.8, 
                                   colsample_bytree=0.8, reg_alpha=0.1, random_state=42, n_jobs=-1)
base_models['LGB'] = LGBMClassifier(n_estimators=800, max_depth=5, learning_rate=0.03, subsample=0.8, 
                                    colsample_bytree=0.8, random_state=42, verbose=-1, n_jobs=-1)
base_models['SVC_rbf'] = SVC(C=2.0, kernel='rbf', probability=True, random_state=42)
base_models['NuSVC'] = NuSVC(probability=True, random_state=42)
base_models['KNN'] = KNeighborsClassifier(n_neighbors=7, weights='distance', n_jobs=-1)
base_models['GNB'] = GaussianNB()
base_models['MLP_deep'] = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32), alpha=0.01, 
                                        learning_rate='adaptive', max_iter=3000, early_stopping=True, random_state=42)
base_models['MLP_wide'] = MLPClassifier(hidden_layer_sizes=(100, 100, 50), alpha=0.005, max_iter=2000, random_state=42)
base_models['Ada'] = AdaBoostClassifier(n_estimators=300, random_state=42)

print(f"‚úÖ {len(base_models)} ultra models loaded")

# =============================================================================
# 3. BENCHMARK ALL MODELS
# =============================================================================
print("\n" + "="*80)
print("üèÜ CROSS-VALIDATION BENCHMARK (15 MODELS)")
print("="*80)

cv_results = {}
for name, model in base_models.items():
    pipe = Pipeline([('prep', preprocessor), ('model', model)])
    scores = cross_val_score(pipe, X_full, y, cv=StratifiedKFold(5), scoring='accuracy', n_jobs=-1)
    cv_results[name] = scores.mean()
    print(f"{name:<15}: {scores.mean():.4f} ¬± {scores.std():.3f}")

top10 = sorted(cv_results.items(), key=lambda x: x[1], reverse=True)[:10]
print(f"\nüî• TOP 10 MODELS:")
for i, (name, score) in enumerate(top10, 1):
    print(f"  {i}. {name:<12}: {score:.4f}")

# =============================================================================
# 4. MULTI-LEVEL POWER ENSEMBLES (FIXED - filter Ridge)
# =============================================================================
print("\n" + "="*80)
print("‚ö° MULTI-LEVEL ENSEMBLES")
print("="*80)

# Filter models with predict_proba
top10_names = [name for name, _ in top10]
voting_models = [(name, base_models[name]) for name in top10_names 
                 if hasattr(base_models[name], 'predict_proba')]

voting_l1 = VotingClassifier(voting_models, voting='soft', n_jobs=-1)
voting_l1_pipe = Pipeline([('prep', preprocessor), ('voting_l1', voting_l1)])
voting_l1_cv = cross_val_score(voting_l1_pipe, X_full, y, cv=5, n_jobs=-1).mean()
print(f"L1 Voting (Top models): {voting_l1_cv:.4f}")

# Stacking (all models OK)
top5_names = [name for name, _ in top10[:5]]
stacking_l2 = StackingClassifier(
    [(name, base_models[name]) for name in top5_names],
    final_estimator=LogisticRegression(max_iter=1000, C=0.1),
    cv=5, n_jobs=-1
)
stacking_l2_pipe = Pipeline([('prep', preprocessor), ('stacking_l2', stacking_l2)])
stacking_l2_cv = cross_val_score(stacking_l2_pipe, X_full, y, cv=5, n_jobs=-1).mean()
print(f"L2 Stacking (Top 5):    {stacking_l2_cv:.4f}")

# =============================================================================
# 5. VALIDATION
# =============================================================================
print("\n" + "="*80)
print("üìä TRAIN/VALIDATION SPLIT")
print("="*80)

X_train, X_val, y_train, y_val = train_test_split(X_full, y, test_size=0.2, random_state=42, stratify=y)

best_pipe = stacking_l2_pipe if stacking_l2_cv >= voting_l1_cv else voting_l1_pipe
best_pipe.fit(X_train, y_train)
val_pred = best_pipe.predict(X_val)
val_acc = accuracy_score(y_val, val_pred)

print(f"Validation Accuracy: {val_acc:.4f}")

# =============================================================================
# 6. FINAL SUBMISSION
# =============================================================================
print("\n" + "="*80)
print("üéØ FINAL TRAINING & SUBMISSION")
print("="*80)

ultimate_model = stacking_l2_pipe
ultimate_model.fit(X_full, y)
final_predictions = ultimate_model.predict(test_full)

submission = pd.DataFrame({
    'PassengerId': test_passenger_id,
    'Survived': final_predictions.astype(int)
})
submission.to_csv('submission_ultimate.csv', index=False)

print(f"\nüèÜ FINAL RESULTS:")
print(f"  Best single model:  {max(cv_results.values()):.4f}")
print(f"  Voting ensemble CV: {voting_l1_cv:.4f}")
print(f"  Stacking CV:        {stacking_l2_cv:.4f}")
print(f"  Validation holdout: {val_acc:.4f}")
print(f"  Expected Kaggle LB: ~{stacking_l2_cv:.3f}")
print(f"\n‚úÖ submission_ultimate.csv SAVED!")
print("üìà Upload to Kaggle now! Expected score: ~0.88")
