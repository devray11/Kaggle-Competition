# -*- coding: utf-8 -*-
"""Kaggle Competition Heart Diseases Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n2JSUMUXErGjWj3cDlKdF41hrs0N3eIw
"""

# 1. Install necessary libraries (Run in Colab cell)
!pip install xgboost lightgbm catboost -q

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.ensemble import HistGradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier, early_stopping, log_evaluation
from catboost import CatBoostClassifier

warnings.filterwarnings('ignore')

# 1. Load data
print("Loading data...")
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# 2. "Heavy" Feature Engineering
def create_ultimate_features(df):
    df = df.copy()

    # Clinical Ratios
    df['Chol_to_Age'] = df['Cholesterol'] / (df['Age'] + 1e-5)
    df['BP_to_MaxHR'] = df['BP'] / (df['Max HR'] + 1e-5)
    df['ST_Dep_Product'] = df['ST depression'] * df['Slope of ST']

    # Boolean Flags for High Risk (The "Depth" comes from these interactions)
    df['HighRisk_Thal'] = df['Thallium'].isin([6, 7]).astype(int)
    df['HighRisk_Vessels'] = (df['Number of vessels fluro'] > 0).astype(int)
    df['Silent_Killer'] = ((df['BP'] > 140) & (df['Exercise angina'] == 0)).astype(int)

    # Numerical-Categorical Interaction
    df['Age_Vessel_Interaction'] = df['Age'] * df['Number of vessels fluro']
    df['HR_Thal_Interaction'] = df['Max HR'] * df['Thallium']

    # Log transforms for skewed data
    df['Log_Chol'] = np.log1p(df['Cholesterol'])
    df['Log_BP'] = np.log1p(df['BP'])

    return df

train = create_ultimate_features(train)
test = create_ultimate_features(test)

X = train.drop(['id', 'Heart Disease'], axis=1)
y = train['Heart Disease'].map({'Absence': 0, 'Presence': 1})
X_test = test.drop(['id'], axis=1)

cat_features = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results',
                'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium']

# 3. Model Definitions (Ultra-Heavy Settings)
# We add HistGradientBoosting - it's like LightGBM but often finds different patterns
hgb_params = {'max_iter': 2000, 'max_depth': 15, 'learning_rate': 0.05, 'l2_regularization': 1.0}

xgb_params = {
    'n_estimators': 3000, 'max_depth': 12, 'learning_rate': 0.02,
    'tree_method': 'hist', 'subsample': 0.9, 'colsample_bytree': 0.7,
    'early_stopping_rounds': 50, 'random_state': 42
}

lgb_params = {
    'n_estimators': 3000, 'num_leaves': 511, 'max_depth': 14,
    'learning_rate': 0.02, 'subsample': 0.8, 'colsample_bytree': 0.8,
    'random_state': 42
}

cat_params = {
    'iterations': 3000, 'depth': 10, 'learning_rate': 0.03,
    'l2_leaf_reg': 6, 'verbose': 0, 'early_stopping_rounds': 50,
    'cat_features': cat_features, 'random_seed': 42
}

# 4. Training with 5-Fold CV
n_folds = 5
kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
oof_preds = np.zeros(len(train))
test_preds = np.zeros(len(test))

print("\nStarting Ultimate Ensemble Training...")

for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # Model 1: XGBoost
    m_xgb = XGBClassifier(**xgb_params)
    m_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
    p_xgb = m_xgb.predict_proba(X_val)[:, 1]

    # Model 2: LightGBM
    m_lgb = LGBMClassifier(**lgb_params)
    m_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)],
              callbacks=[early_stopping(50), log_evaluation(0)])
    p_lgb = m_lgb.predict_proba(X_val)[:, 1]

    # Model 3: CatBoost
    m_cat = CatBoostClassifier(**cat_params)
    m_cat.fit(X_train, y_train, eval_set=[(X_val, y_val)])
    p_cat = m_cat.predict_proba(X_val)[:, 1]

    # Model 4: HistGradientBoosting
    m_hgb = HistGradientBoostingClassifier(**hgb_params)
    m_hgb.fit(X_train, y_train)
    p_hgb = m_hgb.predict_proba(X_val)[:, 1]

    # Dynamic Weighted Blend for this fold
    # We give more weight to XGB and CatBoost as they usua   lly generalize better
    fold_blend = (p_xgb * 0.30) + (p_lgb * 0.20) + (p_cat * 0.30) + (p_hgb * 0.20)
    oof_preds[val_idx] = fold_blend

    # Test Predictions
    test_preds += ((m_xgb.predict_proba(X_test)[:,1] * 0.30) +
                   (m_lgb.predict_proba(X_test)[:,1] * 0.20) +
                   (m_cat.predict_proba(X_test)[:,1] * 0.30) +
                   (m_hgb.predict_proba(X_test)[:,1] * 0.20)) / n_folds

    acc = accuracy_score(y_val, (fold_blend > 0.5).astype(int))
    print(f"Fold {fold+1} Accuracy: {acc:.6f}")

print(f"\nOverall Ensemble OOF Accuracy: {accuracy_score(y, (oof_preds > 0.5).astype(int)):.6f}")

# 5. Save Submission
submission = pd.DataFrame({
    'id': test['id'],
    'Heart Disease': pd.Series((test_preds > 0.5).astype(int)).map({0: 'Absence', 1: 'Presence'})
})
submission.to_csv('ultimate_submission.csv', index=False)
print("Saved: ultimate_submission.csv")

import pandas as pd

# Load your already generated submission
sub = pd.read_csv("/content/ultimate_submission.csv")

# Convert strings to numeric
sub["Heart Disease"] = sub["Heart Disease"].map({
    "Absence": 0,
    "Presence": 1
})

# Ensure correct datatype
sub["Heart Disease"] = sub["Heart Disease"].astype(int)

# Save corrected file
sub.to_csv("submission.csv", index=False)

print("âœ… Fixed file saved as fixed_submission.csv")
print(sub.dtypes)

